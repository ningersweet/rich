#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
===============================================================================
é‡‘å­—å¡”åŠ ä»“ç­–ç•¥å‚æ•°ä¼˜åŒ–è„šæœ¬
===============================================================================

ã€åŠŸèƒ½è¯´æ˜ã€‘
é€šè¿‡ç½‘æ ¼æœç´¢æ–¹æ³•ä¼˜åŒ–é‡‘å­—å¡”åŠ ä»“ç­–ç•¥çš„å‚æ•°ç»„åˆï¼Œå¯»æ‰¾æœ€ä½³å‚æ•°é…ç½®ã€‚

ã€ä¼˜åŒ–å‚æ•°ã€‘
1. pyramid_profit_threshold: ç›ˆåˆ©é˜ˆå€¼ï¼ˆå…è®¸åŠ ä»“çš„æœ€å°ç›ˆåˆ©ç™¾åˆ†æ¯”ï¼‰
2. pyramid_min_rr: åŠ ä»“ä¿¡å·æœ€å°ç›ˆäºæ¯”é˜ˆå€¼
3. pyramid_min_prob: åŠ ä»“ä¿¡å·æœ€å°æ¦‚ç‡é˜ˆå€¼
4. pyramid_max_count: æœ€å¤§åŠ ä»“æ¬¡æ•°
5. pyramid_min_bars: è·ä¸Šæ¬¡åŠ ä»“æœ€å°Kçº¿æ•°
6. max_total_exposure: æ€»æ•å£ä¸Šé™ï¼ˆå«åŠ ä»“ï¼‰

ã€ä¼˜åŒ–ç›®æ ‡ã€‘
- æœ€å¤§åŒ–æ€»æ”¶ç›Šç‡
- æœ€å¤§åŒ–æ”¶ç›Šé£é™©æ¯”ï¼ˆæ€»æ”¶ç›Šç‡/æœ€å¤§å›æ’¤ï¼‰
- åœ¨æ”¶ç›Šå’Œé£é™©ä¹‹é—´å¯»æ‰¾å¹³è¡¡

ã€ä½¿ç”¨æ–¹æ³•ã€‘
cd /Users/lemonshwang/project/rich/lightgbm
python backtest_scripts/pyramid_parameter_optimization.py

ã€ä½œè€…ã€‘Qoder AI
ã€æ—¥æœŸã€‘2026-02-21
===============================================================================
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import pandas as pd
import numpy as np
from pathlib import Path
import logging
from datetime import datetime
from typing import List, Dict, Any, Tuple
import itertools
import json

# å¯¼å…¥å›æµ‹å¼•æ“å’Œæ¨¡å‹
from btc_quant.config import load_config
from btc_quant.data import load_klines
from btc_quant.features import build_features_and_labels
from btc_quant.risk_reward_model import TwoStageRiskRewardStrategy
from backtest_scripts.backtest_engine_pyramid_shared import pyramid_backtest_with_compounding

# æ—¥å¿—é…ç½®
log_file = Path('logs') / f'pyramid_parameter_optimization_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
log_file.parent.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(log_file, encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)
logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file.absolute()}")

# å›æµ‹é…ç½®ï¼ˆå›ºå®šï¼‰
BACKTEST_START = '2025-01-01T00:00:00Z'
BACKTEST_END = '2026-02-20T23:59:59Z'
MODEL_DIR = Path('models/final_2024_dynamic')
INITIAL_BALANCE = 1000.0
MAX_EXPOSURE = 10.0
STOP_LOSS_PCT = -0.03
MAX_DAILY_LOSS_PCT = -0.20
MAX_DRAWDOWN_PAUSE = 0.10
USE_TRAILING_STOP = True
MAX_HOLDING_PERIOD = 20
RR_THRESHOLD = 1.0
PROB_THRESHOLD = 0.0

# é‡‘å­—å¡”å‚æ•°æœç´¢ç©ºé—´ï¼ˆæµ‹è¯•æ¨¡å¼ï¼šæ¯ä¸ªå‚æ•°2ä¸ªå€¼ï¼Œå…±64ç§ç»„åˆï¼‰
PARAM_SEARCH_SPACE = {
    'pyramid_profit_threshold': [0.01, 0.02],            # 1%, 2%
    'pyramid_min_rr': [3.0, 3.5],                        # 3.0, 3.5
    'pyramid_min_prob': [0.75, 0.8],                     # 75%, 80%
    'pyramid_max_count': [3, 4],                         # 3æ¬¡, 4æ¬¡
    'pyramid_min_bars': [5, 7],                          # 5æ ¹, 7æ ¹Kçº¿
    'max_total_exposure': [15.0, 18.0]                   # 15å€, 18å€
}

# å½“å‰æœ€ä½³å‚æ•°é…ç½®ï¼ˆæ¥è‡ªconfig.yamlï¼‰
DEFAULT_PARAMS = {
    'pyramid_enabled': True,
    'pyramid_profit_threshold': 0.01,
    'pyramid_min_rr': 3.0,
    'pyramid_min_prob': 0.75,
    'pyramid_max_count': 3,
    'pyramid_min_bars': 5,
    'max_total_exposure': 15.0
}

def load_data_and_predictions() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    åŠ è½½Kçº¿æ•°æ®å’Œç”Ÿæˆé¢„æµ‹ä¿¡å·ï¼ˆå•æ¬¡åŠ è½½ï¼Œå¤šæ¬¡ä½¿ç”¨ï¼‰
    
    è¿”å›:
        klines_backtest: å›æµ‹Kçº¿æ•°æ®
        predictions: é¢„æµ‹ä¿¡å·DataFrame
    """
    logger.info("=" * 80)
    logger.info("ğŸ“Š åŠ è½½æ•°æ®å’Œç”Ÿæˆé¢„æµ‹ä¿¡å·")
    logger.info("=" * 80)
    
    # åŠ è½½é…ç½®
    cfg = load_config()
    
    logger.info(f"\nå›æµ‹æ—¶é—´èŒƒå›´:{BACKTEST_START}è‡³{BACKTEST_END}")
    logger.info("åŠ è½½Kçº¿æ•°æ®...")
    klines_all = load_klines(cfg)
    klines_all['close_time'] = pd.to_datetime(klines_all['close_time'])
    backtest_start_ts, backtest_end_ts = pd.Timestamp(BACKTEST_START), pd.Timestamp(BACKTEST_END)
    klines_backtest = klines_all[(klines_all['close_time'] >= backtest_start_ts) & 
                                  (klines_all['close_time'] <= backtest_end_ts)].reset_index(drop=True)
    logger.info(f"å›æµ‹é›†Kçº¿æ•°é‡:{len(klines_backtest)}")
    
    logger.info("æ„å»ºç‰¹å¾...")
    feature_label_data = build_features_and_labels(cfg, klines_backtest)
    X_backtest_full = feature_label_data.features.reset_index(drop=True)
    min_len = min(len(X_backtest_full), len(klines_backtest))
    X_backtest_full = X_backtest_full.iloc[:min_len]
    klines_backtest = klines_backtest.iloc[:min_len].reset_index(drop=True)
    logger.info(f"å¯¹é½åæ ·æœ¬æ•°:{len(X_backtest_full)}")
    
    logger.info(f"åŠ è½½æ¨¡å‹:{MODEL_DIR}")
    strategy = TwoStageRiskRewardStrategy()
    strategy.load(MODEL_DIR)
    with open(MODEL_DIR / 'top30_features.txt', 'r') as f:
        top_30_features = [line.strip() for line in f.readlines()]
    logger.info(f"ç‰¹å¾æ•°é‡:{len(top_30_features)}")
    X_backtest_top30 = X_backtest_full[top_30_features]
    
    logger.info("ç”Ÿæˆé¢„æµ‹ä¿¡å·...")
    predictions_dict = strategy.predict(X_backtest_top30, rr_threshold=RR_THRESHOLD, prob_threshold=PROB_THRESHOLD)
    predictions = pd.DataFrame({
        'predicted_rr': predictions_dict['predicted_rr'],
        'direction': predictions_dict['direction'],
        'holding_period': predictions_dict['holding_period'].clip(1, MAX_HOLDING_PERIOD),
        'direction_prob': predictions_dict['direction_prob'],
        'should_trade': predictions_dict['should_trade']
    })
    logger.info(f"æ€»æ ·æœ¬æ•°:{len(predictions)}")
    logger.info(f"åº”äº¤æ˜“æ ·æœ¬:{predictions['should_trade'].sum()}")
    logger.info(f"äº¤æ˜“æ¯”ä¾‹:{predictions['should_trade'].sum()/len(predictions)*100:.2f}%")
    
    logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: Kçº¿{len(klines_backtest)}æ¡, é¢„æµ‹{len(predictions)}æ¡")
    
    return klines_backtest, predictions

def run_single_backtest(
    klines: pd.DataFrame,
    predictions: pd.DataFrame,
    params: Dict[str, Any]
) -> Dict[str, Any]:
    """
    ä½¿ç”¨æŒ‡å®šå‚æ•°è¿è¡Œå•æ¬¡å›æµ‹
    
    å‚æ•°:
        klines: Kçº¿æ•°æ®
        predictions: é¢„æµ‹ä¿¡å·
        params: é‡‘å­—å¡”å‚æ•°å­—å…¸
        
    è¿”å›:
        å›æµ‹ç»“æœå­—å…¸
    """
    try:
        result = pyramid_backtest_with_compounding(
            klines=klines,
            predictions=predictions,
            initial_balance=INITIAL_BALANCE,
            max_total_exposure=params['max_total_exposure'],
            stop_loss_pct=STOP_LOSS_PCT,
            max_daily_loss_pct=MAX_DAILY_LOSS_PCT,
            max_drawdown_pause=MAX_DRAWDOWN_PAUSE,
            use_trailing_stop=USE_TRAILING_STOP,
            pyramid_enabled=params['pyramid_enabled'],
            pyramid_profit_threshold=params['pyramid_profit_threshold'],
            pyramid_min_rr=params['pyramid_min_rr'],
            pyramid_min_prob=params['pyramid_min_prob'],
            pyramid_max_count=params['pyramid_max_count'],
            pyramid_min_bars=params['pyramid_min_bars']
        )
        
        # è®¡ç®—æ”¶ç›Šé£é™©æ¯”
        risk_return_ratio = result['total_return'] / max(result['max_drawdown'], 0.01)
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆæƒé‡ï¼šæ€»æ”¶ç›Šç‡60%ï¼Œæ”¶ç›Šé£é™©æ¯”30%ï¼Œèƒœç‡10%ï¼‰
        # å½’ä¸€åŒ–å¤„ç†ï¼ˆç›¸å¯¹äºæœ€ä¼˜å€¼ï¼‰
        total_return_score = min(result['total_return'] / 1e12, 1.0)  # å‡è®¾æœ€å¤§1ä¸‡äº¿%
        risk_return_score = min(risk_return_ratio / 1e10, 1.0)  # å‡è®¾æœ€å¤§100äº¿
        win_rate_score = result['win_rate'] / 100.0
        
        composite_score = (
            total_return_score * 0.6 +
            risk_return_score * 0.3 +
            win_rate_score * 0.1
        )
        
        # æ·»åŠ é¢å¤–æŒ‡æ ‡
        result['risk_return_ratio'] = risk_return_ratio
        result['composite_score'] = composite_score
        result['params'] = params.copy()
        
        return result
        
    except Exception as e:
        logger.error(f"å‚æ•°ç»„åˆå›æµ‹å¤±è´¥: {params}, é”™è¯¯: {e}")
        # è¿”å›å¤±è´¥ç»“æœ
        return {
            'total_return': -100.0,
            'final_equity': INITIAL_BALANCE,
            'total_trades': 0,
            'win_rate': 0.0,
            'profit_loss_ratio': 0.0,
            'max_drawdown': 100.0,
            'avg_exposure': 0.0,
            'pyramid_trades': 0,
            'risk_return_ratio': -1.0,
            'composite_score': 0.0,
            'params': params,
            'error': str(e)
        }

def generate_parameter_combinations() -> List[Dict[str, Any]]:
    """
    ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
    
    è¿”å›:
        å‚æ•°ç»„åˆåˆ—è¡¨
    """
    param_names = list(PARAM_SEARCH_SPACE.keys())
    param_values = list(PARAM_SEARCH_SPACE.values())
    
    combinations = []
    for values in itertools.product(*param_values):
        params = {
            'pyramid_enabled': True,  # å§‹ç»ˆå¯ç”¨é‡‘å­—å¡”
            **dict(zip(param_names, values))
        }
        combinations.append(params)
    
    logger.info(f"ç”Ÿæˆäº† {len(combinations)} ç§å‚æ•°ç»„åˆ")
    return combinations

def run_optimization() -> pd.DataFrame:
    """
    è¿è¡Œå‚æ•°ä¼˜åŒ–ä¸»æµç¨‹
    
    è¿”å›:
        åŒ…å«æ‰€æœ‰å›æµ‹ç»“æœçš„DataFrame
    """
    logger.info("=" * 80)
    logger.info("ğŸš€ å¼€å§‹é‡‘å­—å¡”å‚æ•°ä¼˜åŒ–")
    logger.info("=" * 80)
    
    # 1. åŠ è½½æ•°æ®ï¼ˆå•æ¬¡åŠ è½½ï¼Œé¿å…é‡å¤è®¡ç®—ï¼‰
    logger.info("\næ­¥éª¤1: åŠ è½½æ•°æ®å’Œç”Ÿæˆé¢„æµ‹ä¿¡å·...")
    klines, predictions = load_data_and_predictions()
    
    # 2. ç”Ÿæˆå‚æ•°ç»„åˆ
    logger.info("\næ­¥éª¤2: ç”Ÿæˆå‚æ•°ç»„åˆ...")
    param_combinations = generate_parameter_combinations()
    
    # 3. æ·»åŠ é»˜è®¤å‚æ•°ç»„åˆä½œä¸ºåŸºå‡†
    param_combinations.insert(0, DEFAULT_PARAMS)
    logger.info(f"æ€»å‚æ•°ç»„åˆæ•°ï¼ˆå«åŸºå‡†ï¼‰: {len(param_combinations)}")
    
    # 4. è¿è¡Œå›æµ‹
    logger.info("\næ­¥éª¤3: è¿è¡Œå›æµ‹...")
    results = []
    
    for i, params in enumerate(param_combinations):
        logger.info(f"\n[{i+1}/{len(param_combinations)}] æµ‹è¯•å‚æ•°ç»„åˆ:")
        logger.info(f"  ç›ˆåˆ©é˜ˆå€¼: {params['pyramid_profit_threshold']*100:.1f}%")
        logger.info(f"  æœ€å°RR: {params['pyramid_min_rr']:.1f}")
        logger.info(f"  æœ€å°æ¦‚ç‡: {params['pyramid_min_prob']:.2f}")
        logger.info(f"  æœ€å¤§åŠ ä»“æ¬¡æ•°: {params['pyramid_max_count']}")
        logger.info(f"  æœ€å°Kçº¿é—´éš”: {params['pyramid_min_bars']}")
        logger.info(f"  æ€»æ•å£ä¸Šé™: {params['max_total_exposure']:.1f}å€")
        
        result = run_single_backtest(klines, predictions, params)
        results.append(result)
        
        # æ˜¾ç¤ºå…³é”®ç»“æœ
        logger.info(f"  ç»“æœ: æ”¶ç›Šç‡={result['total_return']:.2f}%, "
                   f"é£é™©æ”¶ç›Šæ¯”={result.get('risk_return_ratio', 0):.2f}, "
                   f"ç»¼åˆè¯„åˆ†={result.get('composite_score', 0):.4f}")
    
    # 5. è½¬æ¢ä¸ºDataFrameå¹¶æ’åº
    logger.info("\næ­¥éª¤4: åˆ†æç»“æœ...")
    results_df = pd.DataFrame(results)
    
    # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
    results_df = results_df.sort_values('composite_score', ascending=False).reset_index(drop=True)
    
    return results_df

def analyze_results(results_df: pd.DataFrame) -> Dict[str, Any]:
    """
    åˆ†æä¼˜åŒ–ç»“æœ
    
    å‚æ•°:
        results_df: å›æµ‹ç»“æœDataFrame
        
    è¿”å›:
        åˆ†æç»“æœå­—å…¸
    """
    logger.info("=" * 80)
    logger.info("ğŸ“Š ä¼˜åŒ–ç»“æœåˆ†æ")
    logger.info("=" * 80)
    
    if len(results_df) == 0:
        logger.error("æ²¡æœ‰å¯åˆ†æçš„ç»“æœ")
        return {}
    
    # 1. æœ€ä½³å‚æ•°ç»„åˆ
    best_result = results_df.iloc[0]
    best_params = best_result['params']
    
    logger.info("\nğŸ¯ æœ€ä½³å‚æ•°ç»„åˆ (ç»¼åˆè¯„åˆ†æœ€é«˜):")
    logger.info(f"  ç»¼åˆè¯„åˆ†: {best_result['composite_score']:.4f}")
    logger.info(f"  æ€»æ”¶ç›Šç‡: {best_result['total_return']:.2f}%")
    logger.info(f"  é£é™©æ”¶ç›Šæ¯”: {best_result.get('risk_return_ratio', 0):.2f}")
    logger.info(f"  èƒœç‡: {best_result['win_rate']:.2f}%")
    logger.info(f"  ç›ˆäºæ¯”: {best_result['profit_loss_ratio']:.2f}")
    logger.info(f"  æœ€å¤§å›æ’¤: {best_result['max_drawdown']:.2f}%")
    logger.info(f"  åŠ ä»“äº¤æ˜“æ•°: {best_result['pyramid_trades']}")
    
    logger.info("\nğŸ“‹ æœ€ä½³å‚æ•°é…ç½®:")
    for key, value in best_params.items():
        if key != 'pyramid_enabled':
            logger.info(f"  {key}: {value}")
    
    # 2. åŸºå‡†å‚æ•°ç»“æœï¼ˆé»˜è®¤é…ç½®ï¼‰
    baseline_mask = results_df.apply(lambda row: all(
        row['params'].get(k) == v for k, v in DEFAULT_PARAMS.items() if k != 'pyramid_enabled'
    ), axis=1)
    
    if baseline_mask.any():
        baseline_result = results_df[baseline_mask].iloc[0]
        logger.info("\nğŸ“ˆ åŸºå‡†å‚æ•°ç»“æœ (å½“å‰é…ç½®):")
        logger.info(f"  ç»¼åˆè¯„åˆ†: {baseline_result['composite_score']:.4f}")
        logger.info(f"  æ€»æ”¶ç›Šç‡: {baseline_result['total_return']:.2f}%")
        logger.info(f"  é£é™©æ”¶ç›Šæ¯”: {baseline_result.get('risk_return_ratio', 0):.2f}")
        
        # è®¡ç®—æå‡å¹…åº¦
        improvement_pct = (best_result['composite_score'] - baseline_result['composite_score']) / baseline_result['composite_score'] * 100
        logger.info(f"  ç»¼åˆè¯„åˆ†æå‡: {improvement_pct:.1f}%")
    
    # 3. å‚æ•°æ•æ„Ÿæ€§åˆ†æ
    logger.info("\nğŸ”¬ å‚æ•°æ•æ„Ÿæ€§åˆ†æ:")
    param_importance = {}
    
    for param_name in PARAM_SEARCH_SPACE.keys():
        param_values = []
        scores = []
        
        for _, row in results_df.iterrows():
            param_value = row['params'].get(param_name)
            if param_value is not None:
                param_values.append(param_value)
                scores.append(row['composite_score'])
        
        if param_values:
            # è®¡ç®—æ¯ä¸ªå‚æ•°å€¼çš„å¹³å‡å¾—åˆ†
            unique_values = sorted(set(param_values))
            value_scores = []
            for val in unique_values:
                val_scores = [s for p, s in zip(param_values, scores) if p == val]
                if val_scores:
                    value_scores.append((val, np.mean(val_scores)))
            
            if value_scores:
                # æ‰¾åˆ°æœ€ä½³å‚æ•°å€¼
                best_val, best_score = max(value_scores, key=lambda x: x[1])
                worst_val, worst_score = min(value_scores, key=lambda x: x[1])
                
                sensitivity = (best_score - worst_score) / worst_score * 100 if worst_score > 0 else 0
                param_importance[param_name] = sensitivity
                
                logger.info(f"  {param_name}:")
                logger.info(f"    æœ€ä½³å€¼: {best_val} (å¾—åˆ†: {best_score:.4f})")
                logger.info(f"    æœ€å·®å€¼: {worst_val} (å¾—åˆ†: {worst_score:.4f})")
                logger.info(f"    æ•æ„Ÿåº¦: {sensitivity:.1f}%")
    
    # 4. ä¿å­˜ç»“æœ
    output_dir = Path('backtest/parameter_optimization')
    output_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_results = results_df.copy()
    detailed_results['params_json'] = detailed_results['params'].apply(json.dumps)
    detailed_results = detailed_results.drop(columns=['params'])
    
    output_file = output_dir / f'pyramid_optimization_results_{timestamp}.csv'
    detailed_results.to_csv(output_file, index=False)
    logger.info(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    
    # ä¿å­˜æœ€ä½³å‚æ•°é…ç½®
    best_config_file = output_dir / f'best_pyramid_config_{timestamp}.json'
    with open(best_config_file, 'w', encoding='utf-8') as f:
        json.dump(best_params, f, indent=2, ensure_ascii=False)
    logger.info(f"ğŸ“ æœ€ä½³å‚æ•°é…ç½®å·²ä¿å­˜è‡³: {best_config_file}")
    
    # 5. ç”Ÿæˆå»ºè®®
    logger.info("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾æ”¹è¿›
    if 'improvement_pct' in locals() and improvement_pct > 5.0:
        logger.info("  1. âœ… å‘ç°æ˜¾è‘—æ›´å¥½çš„å‚æ•°ç»„åˆï¼Œå»ºè®®æ›´æ–°é…ç½®")
        logger.info(f"  2. ğŸ“ˆ é¢„æœŸæå‡: {improvement_pct:.1f}%")
    else:
        logger.info("  1. âš ï¸  å½“å‰å‚æ•°é…ç½®å·²æ¥è¿‘æœ€ä¼˜ï¼Œæå‡ç©ºé—´æœ‰é™")
    
    # å‚æ•°è°ƒæ•´å»ºè®®
    logger.info("  3. ğŸ›ï¸  å‚æ•°è°ƒæ•´æ–¹å‘:")
    for param_name, sensitivity in sorted(param_importance.items(), key=lambda x: x[1], reverse=True):
        if sensitivity > 10.0:  # é«˜æ•æ„Ÿåº¦å‚æ•°
            current_val = DEFAULT_PARAMS.get(param_name)
            best_val = best_params.get(param_name)
            if current_val != best_val:
                logger.info(f"    - {param_name}: {current_val} â†’ {best_val} (æ•æ„Ÿåº¦: {sensitivity:.1f}%)")
    
    return {
        'best_params': best_params,
        'best_result': best_result.to_dict(),
        'param_importance': param_importance,
        'output_files': {
            'detailed_results': str(output_file),
            'best_config': str(best_config_file)
        }
    }

def main():
    """ä¸»å‡½æ•°"""
    logger.info(__doc__)
    
    try:
        # è¿è¡Œä¼˜åŒ–
        results_df = run_optimization()
        
        # åˆ†æç»“æœ
        analysis = analyze_results(results_df)
        
        logger.info("\n" + "=" * 80)
        logger.info("âœ… å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        logger.info("=" * 80)
        
        # æ˜¾ç¤ºæœ€ä½³å‚æ•°é…ç½®ï¼ˆå¯ç›´æ¥å¤åˆ¶åˆ°config.yamlï¼‰
        if analysis:
            best_params = analysis.get('best_params', {})
            logger.info("\nğŸ“‹ æœ€ä½³å‚æ•°é…ç½® (å¯ç›´æ¥å¤åˆ¶åˆ°config.yamlçš„enhancedéƒ¨åˆ†):")
            logger.info("  # é‡‘å­—å¡”åŠ ä»“é…ç½® (ä¼˜åŒ–å)")
            for key, value in best_params.items():
                if key != 'pyramid_enabled':
                    logger.info(f"  {key}: {value}")
        
    except Exception as e:
        logger.error(f"å‚æ•°ä¼˜åŒ–å¤±è´¥: {e}", exc_info=True)
        raise

if __name__ == '__main__':
    main()