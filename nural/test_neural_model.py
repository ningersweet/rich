"""æµ‹è¯•ç¥ç»ç½‘ç»œæ¨¡å‹åŸºæœ¬åŠŸèƒ½"""
import torch
import numpy as np
import pandas as pd
from btc_quant.neural_model import (
    LSTMFeatureExtractor,
    TransformerFeatureExtractor,
    HybridNeuralModel,
    create_sequences,
)

print("="*60)
print("ç¥ç»ç½‘ç»œæ¨¡å‹åŠŸèƒ½æµ‹è¯•")
print("="*60)

# æµ‹è¯•å‚æ•°
batch_size = 4
seq_len = 20
input_dim = 10
hidden_dim = 32
output_dim = 16

print(f"\næµ‹è¯•é…ç½®:")
print(f"  Batch Size: {batch_size}")
print(f"  Sequence Length: {seq_len}")
print(f"  Input Dim: {input_dim}")
print(f"  Hidden Dim: {hidden_dim}")
print(f"  Output Dim: {output_dim}")

# ========== æµ‹è¯•1: LSTMç‰¹å¾æå–å™¨ ==========
print("\n" + "-"*60)
print("ã€æµ‹è¯•1ã€‘LSTMç‰¹å¾æå–å™¨")
print("-"*60)

lstm_model = LSTMFeatureExtractor(
    input_dim=input_dim,
    hidden_dim=hidden_dim,
    output_dim=output_dim,
)

# åˆ›å»ºéšæœºè¾“å…¥
x_lstm = torch.randn(batch_size, seq_len, input_dim)
print(f"è¾“å…¥å½¢çŠ¶: {x_lstm.shape}")

# å‰å‘ä¼ æ’­
lstm_output = lstm_model(x_lstm)
print(f"è¾“å‡ºå½¢çŠ¶: {lstm_output.shape}")
print(f"âœ… LSTMæ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")

# ========== æµ‹è¯•2: Transformerç‰¹å¾æå–å™¨ ==========
print("\n" + "-"*60)
print("ã€æµ‹è¯•2ã€‘Transformerç‰¹å¾æå–å™¨")
print("-"*60)

transformer_model = TransformerFeatureExtractor(
    input_dim=input_dim,
    d_model=hidden_dim,
    nhead=4,
    output_dim=output_dim,
)

# åˆ›å»ºéšæœºè¾“å…¥
x_transformer = torch.randn(batch_size, seq_len, input_dim)
print(f"è¾“å…¥å½¢çŠ¶: {x_transformer.shape}")

# å‰å‘ä¼ æ’­
transformer_output = transformer_model(x_transformer)
print(f"è¾“å‡ºå½¢çŠ¶: {transformer_output.shape}")
print(f"âœ… Transformeræ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")

# ========== æµ‹è¯•3: æ··åˆæ¨¡å‹ ==========
print("\n" + "-"*60)
print("ã€æµ‹è¯•3ã€‘æ··åˆç¥ç»ç½‘ç»œæ¨¡å‹")
print("-"*60)

# æµ‹è¯•LSTMç‰ˆæœ¬
hybrid_lstm = HybridNeuralModel(
    input_dim=input_dim,
    num_classes=3,
    model_type="lstm",
    hidden_dim=hidden_dim,
    output_dim=output_dim,
)

logits = hybrid_lstm(x_lstm)
print(f"LSTMæ··åˆæ¨¡å‹è¾“å‡º: {logits.shape}")
print(f"âœ… LSTMæ··åˆæ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")

# æµ‹è¯•Transformerç‰ˆæœ¬
hybrid_transformer = HybridNeuralModel(
    input_dim=input_dim,
    num_classes=3,
    model_type="transformer",
    hidden_dim=hidden_dim,
    output_dim=output_dim,
)

logits = hybrid_transformer(x_transformer)
print(f"Transformeræ··åˆæ¨¡å‹è¾“å‡º: {logits.shape}")
print(f"âœ… Transformeræ··åˆæ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")

# ========== æµ‹è¯•4: åºåˆ—åˆ›å»º ==========
print("\n" + "-"*60)
print("ã€æµ‹è¯•4ã€‘æ—¶åºåºåˆ—åˆ›å»º")
print("-"*60)

# åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
n_samples = 100
n_features = 5
sequence_length = 10

features = pd.DataFrame(
    np.random.randn(n_samples, n_features),
    columns=[f"feat_{i}" for i in range(n_features)]
)
labels = pd.Series(np.random.choice([-1, 0, 1], size=n_samples))

print(f"åŸå§‹ç‰¹å¾: {features.shape}")
print(f"åŸå§‹æ ‡ç­¾: {labels.shape}")

# åˆ›å»ºåºåˆ—
X_seq, y_seq = create_sequences(features, labels, sequence_length)
print(f"åºåˆ—åŒ–ç‰¹å¾: {X_seq.shape}")
print(f"åºåˆ—åŒ–æ ‡ç­¾: {y_seq.shape}")
print(f"âœ… åºåˆ—åˆ›å»ºæµ‹è¯•é€šè¿‡ï¼")

# ========== æµ‹è¯•5: ç«¯åˆ°ç«¯è®­ç»ƒæµ‹è¯• ==========
print("\n" + "-"*60)
print("ã€æµ‹è¯•5ã€‘ç«¯åˆ°ç«¯è®­ç»ƒï¼ˆMini Testï¼‰")
print("-"*60)

# åˆ›å»ºå°è§„æ¨¡æ•°æ®
train_size = 200
test_size = 50

train_features = pd.DataFrame(
    np.random.randn(train_size, n_features),
    columns=[f"feat_{i}" for i in range(n_features)]
)
train_labels = pd.Series(np.random.choice([-1, 0, 1], size=train_size))

# æ ‡ç­¾æ˜ å°„
train_labels_mapped = train_labels + 1

# åˆ›å»ºåºåˆ—
X_train, y_train = create_sequences(train_features, train_labels_mapped, sequence_length)

# è½¬æ¢ä¸ºTensor
X_train_t = torch.from_numpy(X_train)
y_train_t = torch.from_numpy(y_train)

# åˆ›å»ºæ¨¡å‹
model = HybridNeuralModel(
    input_dim=n_features,
    num_classes=3,
    model_type="lstm",
)

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒå‡ è½®
epochs = 5
batch_size = 32

print(f"è®­ç»ƒæ•°æ®: {X_train.shape}")
print(f"å¼€å§‹è®­ç»ƒ {epochs} è½®...")

for epoch in range(epochs):
    model.train()
    epoch_loss = 0.0
    
    for i in range(0, len(X_train_t), batch_size):
        batch_X = X_train_t[i:i+batch_size]
        batch_y = y_train_t[i:i+batch_size]
        
        optimizer.zero_grad()
        logits = model(batch_X)
        loss = criterion(logits, batch_y)
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
    
    avg_loss = epoch_loss / (len(X_train_t) // batch_size + 1)
    print(f"  Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}")

print(f"âœ… ç«¯åˆ°ç«¯è®­ç»ƒæµ‹è¯•é€šè¿‡ï¼")

# ========== æ€»ç»“ ==========
print("\n" + "="*60)
print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç¥ç»ç½‘ç»œæ¨¡å‹åŠŸèƒ½æ­£å¸¸")
print("="*60)
print("\nå¯ä»¥å¼€å§‹è®­ç»ƒå®Œæ•´çš„æ··åˆæ¨¡å‹:")
print("  python train_hybrid_model.py")
